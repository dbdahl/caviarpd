% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/caviarPD.R
\name{caviarPD}
\alias{caviarPD}
\title{Cluster Analysis via Random Partition Distributions}
\usage{
caviarPD(
  distance,
  temperature = 10,
  mass = 1,
  discount = 0,
  loss = "VI",
  nSamples = 100,
  maxSize = 0,
  samplesOnly = FALSE,
  distr = "EPA",
  nCores = 0
)
}
\arguments{
\item{distance}{A pairwise distance matrix of class 'dist'.}

\item{temperature}{A positive number that accentuates or dampens distance between observations.}

\item{mass}{The main tuning parameter; higher mass leads to more clusters.}

\item{discount}{Typically 0, controls the distribution of subset sizes.}

\item{loss}{The salso method aims to estimate this loss function when searching the partition space for an optimal estimate, must be specified as either "binder" or "VI".}

\item{nSamples}{The number of samples used to estimate the loss function in the salso method.}

\item{maxSize}{Restriction parameter for the maximum number of clusters in the clustering estimate.}

\item{samplesOnly}{If TRUE, returns only the samples generated for a given mass, temperature, and discount rather than an actual clustering estimate.}

\item{distr}{The random partition distribution used to generate samples, must be specified as either "EPA" or "ddCRP".}

\item{nCores}{The number of CPU cores to use. A value of zero indicates to use all cores on the system.}
}
\value{
A list containing two elements: the clustering estimate and the summary. The summary contains the pairwise probabilities for all samples and is used to create the confidence plots.
}
\description{
Returns a clustering estimate given pairwise distances. Users can also specify parameters for the mass, discount, temperature, loss function, or number of samples.
}
\examples{
iris.dis <- dist(iris[,-5])
caviarPD(distance=iris.dis, nSamples=10)
caviarPD(distance=iris.dis, mass=0.75, loss="binder", nSamples=10, maxSize=3)
# In practice the user should use at least 100 samples, but for ease of testing we use less here.

}
